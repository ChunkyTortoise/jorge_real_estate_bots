"""
Lead Analyzer Service - Enhanced with Production Features.

AI-powered lead qualification using Claude with Jorge's business rules.
Target: <500ms analysis time, >85% accuracy.

Production enhancements from jorge_deployment_package:
- PerformanceCache for <100ms cache hits
- PerformanceMetrics tracking for 5-minute rule compliance
- Hybrid pattern + AI analysis approach
- Jorge's business rules validation integration
"""
import json
import re
import time
from typing import Any, Dict, Optional, Tuple

from bots.shared.business_rules import JorgeBusinessRules
from bots.shared.cache_service import CacheService, PerformanceCache
from bots.shared.claude_client import ClaudeClient, TaskComplexity
from bots.shared.config import settings
from bots.shared.event_broker import event_broker
from bots.shared.ghl_client import GHLClient
from bots.shared.logger import get_logger
from bots.shared.models import PerformanceMetrics
from database.repository import upsert_contact, upsert_lead

logger = get_logger(__name__)


class LeadAnalyzer:
    """
    Lead qualification engine with Claude AI - Enhanced with Production Features.

    Analyzes leads against Jorge's criteria:
    - Price range: $200K-$800K
    - Service areas: Rancho Cucamonga, Ontario, Upland, Fontana, Chino Hills
    - Timeline: Preferably <60 days
    - Buyer/Seller motivation assessment

    Production enhancements:
    - <100ms cache hits via PerformanceCache
    - <500ms AI analysis target
    - 5-minute rule compliance monitoring
    - Jorge's business rules validation
    - Hybrid pattern + AI approach
    """

    def __init__(self):
        """Initialize Lead Analyzer with clients and performance cache."""
        self.claude = ClaudeClient()
        self.ghl = GHLClient()
        self.cache = CacheService()
        self.performance_cache = PerformanceCache(ttl_seconds=300)  # 5-minute cache
        logger.info("LeadAnalyzer initialized with performance cache")

    async def analyze_lead(
        self,
        lead_data: Dict[str, Any],
        force_ai: bool = False
    ) -> Tuple[Dict[str, Any], PerformanceMetrics]:
        """
        Enhanced lead analysis with performance optimization and Jorge's validation.

        Production enhancements:
        - Performance metrics tracking
        - Cache checking for <100ms responses
        - Pattern-based quick scoring for simple leads
        - Jorge's business rules validation
        - 5-minute rule monitoring

        Args:
            lead_data: GHL webhook payload with contact data
            force_ai: Force AI analysis even if cached/pattern match available

        Returns:
            Tuple of (analysis_dict, performance_metrics)
            - analysis_dict: Score, temperature, reasoning, Jorge validation
            - performance_metrics: Timing and compliance data
        """
        metrics = PerformanceMetrics(start_time=time.time())
        contact_id = lead_data.get("id")

        # Create cache key from lead data
        message = self._extract_message_for_cache(lead_data)
        context = {"contact_id": contact_id, "source": lead_data.get("source")}

        logger.info(f"ðŸ” Analyzing lead: {contact_id}")

        # Emit lead analysis started event
        try:
            await event_broker.publish_lead_event(
                "lead.analyzed",
                contact_id=contact_id or "unknown",
                score=0,  # Will be updated when complete
                temperature="unknown",
                jorge_priority="normal",
                estimated_commission=0.0,
                meets_jorge_criteria=False,
                analysis_time_ms=0,
                cache_hit=False
            )
        except Exception as e:
            logger.warning(f"Failed to publish lead analysis started event: {e}")

        try:
            # Check performance cache first for <100ms responses
            if not force_ai:
                cached_result = await self.performance_cache.get(message, context)
                if cached_result:
                    metrics.cache_hit = True
                    metrics.total_time = time.time() - metrics.start_time
                    metrics.analysis_type = "cached"

                    # Add Jorge validation if not already present
                    if "jorge_validation" not in cached_result:
                        cached_result.update(self._add_jorge_validation(cached_result))

                    logger.info(f"âš¡ Cache hit - {metrics.total_time*1000:.0f}ms")

                    # Emit cache hit event
                    try:
                        await event_broker.publish_lead_event(
                            "lead.cache_hit",
                            contact_id=contact_id or "unknown",
                            cache_key=message[:100],  # Truncate for privacy
                            response_time_ms=metrics.total_time * 1000
                        )
                    except Exception as e:
                        logger.warning(f"Failed to publish cache hit event: {e}")

                    return cached_result, metrics

            # Perform AI-powered analysis (existing logic)
            analysis = await self._analyze_with_ai(lead_data, metrics)

            # Add Jorge's business rules validation
            analysis.update(self._add_jorge_validation(analysis))

            # Check 5-minute rule compliance
            metrics.total_time = time.time() - metrics.start_time
            metrics.five_minute_rule_compliant = metrics.total_time < 300  # 5 minutes

            # Performance warnings
            if metrics.total_time > 5.0:
                logger.warning(f"âš ï¸  Slow analysis: {metrics.total_time*1000:.0f}ms")

            if metrics.total_time > 300:
                logger.error(f"ðŸš¨ CRITICAL: 5-minute rule violated! {metrics.total_time:.1f}s")

            # Cache result for future requests
            await self.performance_cache.set(message, analysis, context)

            # Persist to database
            await self._persist_lead(contact_id, lead_data, analysis)

            logger.info(f"âœ… Analysis complete - {metrics.analysis_type}: {metrics.total_time*1000:.0f}ms")

            # Emit lead analysis complete event
            try:
                await event_broker.publish_lead_event(
                    "lead.analyzed",
                    contact_id=contact_id or "unknown",
                    score=analysis.get("score", 0),
                    temperature=analysis.get("temperature", "warm"),
                    jorge_priority=analysis.get("jorge_priority", "normal"),
                    estimated_commission=analysis.get("estimated_commission", 0.0),
                    meets_jorge_criteria=analysis.get("meets_jorge_criteria", False),
                    analysis_time_ms=metrics.total_time * 1000,
                    cache_hit=False
                )

                # Emit hot lead detected event if temperature is hot
                if analysis.get("temperature") == "hot":
                    await event_broker.publish_lead_event(
                        "lead.hot_detected",
                        contact_id=contact_id or "unknown",
                        score=analysis.get("score", 0),
                        estimated_commission=analysis.get("estimated_commission", 0.0),
                        hot_indicators=analysis.get("hot_indicators", ["high_score"])
                    )

            except Exception as e:
                logger.warning(f"Failed to publish lead analysis complete event: {e}")

            return analysis, metrics

        except Exception as e:
            logger.error(f"âŒ Lead analysis error: {e}")

            # Emit error event
            try:
                await event_broker.publish_lead_event(
                    "lead.error",
                    contact_id=contact_id or "unknown",
                    error_type=type(e).__name__,
                    error_message=str(e),
                    stack_trace=None  # Could add traceback if needed
                )
            except Exception as event_error:
                logger.warning(f"Failed to publish error event: {event_error}")

            # Fallback analysis
            fallback = self._fallback_scoring(lead_data)
            fallback["error"] = str(e)
            fallback["fallback_used"] = True
            fallback.update(self._add_jorge_validation(fallback))

            metrics.total_time = time.time() - metrics.start_time
            metrics.analysis_type = "fallback"

            await self._persist_lead(contact_id, lead_data, fallback)
            return fallback, metrics

    def _extract_message_for_cache(self, lead_data: Dict[str, Any]) -> str:
        """Extract key message content for cache key generation."""
        parts = []

        if lead_data.get("name"):
            parts.append(lead_data["name"])
        if lead_data.get("email"):
            parts.append(lead_data["email"])
        if lead_data.get("phone"):
            parts.append(lead_data["phone"])
        if lead_data.get("tags"):
            parts.append(",".join(lead_data["tags"]))

        # Include custom fields that affect scoring
        custom = lead_data.get("customField", {})
        if custom.get("budget"):
            parts.append(f"budget:{custom['budget']}")
        if custom.get("timeline"):
            parts.append(f"timeline:{custom['timeline']}")
        if custom.get("location"):
            parts.append(f"location:{custom['location']}")

        return " | ".join(parts)

    async def _analyze_with_ai(
        self,
        lead_data: Dict[str, Any],
        metrics: PerformanceMetrics
    ) -> Dict[str, Any]:
        """
        Perform AI-powered analysis (original logic).

        Preserves existing MVP behavior while adding metrics tracking.
        """
        contact_id = lead_data.get("id")
        logger.info(f"Analyzing lead: {contact_id}")

        # Extract relevant fields
        name = lead_data.get("name", "Unknown")
        email = lead_data.get("email", "")
        phone = lead_data.get("phone", "")
        source = lead_data.get("source", "Unknown")
        tags = lead_data.get("tags", [])
        custom_fields = lead_data.get("customField", {})

        # Build analysis prompt
        prompt = self._build_analysis_prompt(lead_data)

        # Call Claude AI for analysis (Target: <500ms)
        ai_start = time.time()

        response = await self.claude.agenerate(
            prompt=prompt,
            system_prompt=self._get_system_prompt(),
            complexity=TaskComplexity.COMPLEX,  # Lead qualification is complex
            max_tokens=500,
            temperature=0.3,  # Low temperature for consistent scoring
            enable_caching=True  # Cache system prompt
        )

        metrics.claude_analysis_time = time.time() - ai_start
        metrics.analysis_type = "ai"

        # Parse response
        analysis = self._parse_claude_response(response.content)

        # Extract budget/location for Jorge validation
        analysis = self._extract_lead_details(analysis, lead_data)

        # Update GHL with results (non-blocking)
        await self._update_ghl_fields(contact_id, analysis)

        # Send immediate follow-up based on temperature (non-blocking)
        await self._send_followup(contact_id, analysis)

        return analysis

    def _extract_lead_details(self, analysis: Dict, lead_data: Dict) -> Dict:
        """
        Extract budget and location details for Jorge validation.

        Parses budget_estimate and extracts location from custom fields.
        """
        # Extract budget range from estimate
        budget_estimate = analysis.get("budget_estimate")
        if budget_estimate and isinstance(budget_estimate, str):
            # Try to parse budget (e.g., "$400K-$600K" or "$500,000")
            numbers = re.findall(r'\$?([\d,]+)K?', budget_estimate.replace(',', ''))
            if numbers:
                # Convert to actual numbers (handle K suffix)
                parsed_numbers = []
                for num in numbers:
                    val = int(num)
                    if 'K' in budget_estimate or 'k' in budget_estimate:
                        if val < 10000:  # If it's like "400K", multiply by 1000
                            val = val * 1000
                    parsed_numbers.append(val)

                if len(parsed_numbers) == 2:
                    analysis["budget_min"] = min(parsed_numbers)
                    analysis["budget_max"] = max(parsed_numbers)
                elif len(parsed_numbers) == 1:
                    analysis["budget_max"] = parsed_numbers[0]

        # Extract location preferences
        custom_fields = lead_data.get("customField", {})
        location = custom_fields.get("location", "")
        if location:
            analysis["location_preferences"] = [location]
        elif lead_data.get("tags"):
            # Check tags for location keywords
            analysis["location_preferences"] = [
                tag for tag in lead_data.get("tags", [])
                if any(area.lower() in tag.lower() for area in JorgeBusinessRules.SERVICE_AREAS)
            ]

        return analysis

    def _add_jorge_validation(self, analysis: Dict) -> Dict:
        """
        Add Jorge's business rules validation to analysis.

        Integrates production validation logic from jorge_claude_intelligence.py.
        """
        # Extract relevant data for validation
        lead_validation_data = {
            "budget_max": analysis.get("budget_max"),
            "location_preferences": analysis.get("location_preferences", []),
            "timeline": analysis.get("timeline_estimate", "unknown")
        }

        # Apply Jorge's business rules
        jorge_validation = JorgeBusinessRules.validate_lead(lead_validation_data)

        # Return validation results
        return {
            "jorge_validation": jorge_validation,
            "meets_jorge_criteria": jorge_validation["passes_jorge_criteria"],
            "estimated_commission": jorge_validation["estimated_commission"],
            "jorge_priority": jorge_validation["jorge_priority"],
            "service_area_match": jorge_validation["service_area_match"]
        }

    def _get_system_prompt(self) -> str:
        """
        Get system prompt for lead analysis.

        This prompt is cached by Claude (90% token savings on subsequent calls).
        """
        return f"""You are Jorge's AI Lead Qualification Assistant for real estate.

**Jorge's Business Rules:**
- Price Range: ${settings.jorge_min_price:,} - ${settings.jorge_max_price:,}
- Service Areas: {settings.jorge_service_areas}
- Preferred Timeline: {settings.jorge_preferred_timeline} days or less
- Commission: {settings.jorge_standard_commission * 100}% (negotiable to {settings.jorge_minimum_commission * 100}%)

**Lead Scoring Criteria (0-100):**

1. **Price Range Match (30 points)**
   - Within range: 30 points
   - Slightly outside: 20 points
   - Way outside: 0 points

2. **Location (25 points)**
   - In service area: 25 points
   - Adjacent area: 15 points
   - Outside: 0 points

3. **Timeline/Urgency (20 points)**
   - ASAP/30 days: 20 points
   - 60-90 days: 15 points
   - 6+ months: 5 points

4. **Buyer Motivation (15 points)**
   - Strong signals (pre-approved, selling home): 15 points
   - Medium (actively looking): 10 points
   - Weak (just browsing): 5 points

5. **Contact Quality (10 points)**
   - Full contact info + specific needs: 10 points
   - Partial info: 5 points
   - Minimal: 2 points

**Temperature Assignment:**
- **HOT (80-100)**: Immediate call within 1 hour
- **WARM (60-79)**: Follow up within 24 hours
- **COLD (0-59)**: Add to nurture sequence

**Response Format (JSON):**
{{
  "score": <0-100>,
  "temperature": "hot|warm|cold",
  "reasoning": "Brief explanation",
  "action": "Recommended next step",
  "budget_estimate": "<min-max range or null>",
  "timeline_estimate": "<days or null>"
}}

Be concise. Focus on actionable insights for Jorge."""

    def _build_analysis_prompt(self, lead_data: Dict[str, Any]) -> str:
        """Build analysis prompt from lead data."""
        name = lead_data.get("name", "Unknown")
        email = lead_data.get("email", "")
        phone = lead_data.get("phone", "")
        source = lead_data.get("source", "Unknown")
        tags = lead_data.get("tags", [])
        custom_fields = lead_data.get("customField", {})

        prompt = f"""Analyze this lead:

**Contact:**
- Name: {name}
- Email: {email}
- Phone: {phone}
- Source: {source}
- Tags: {', '.join(tags) if tags else 'None'}

**Custom Fields:**
"""
        for key, value in custom_fields.items():
            prompt += f"- {key}: {value}\n"

        prompt += "\nProvide lead score, temperature, and recommended action in JSON format."

        return prompt

    def _parse_claude_response(self, response_text: str) -> Dict[str, Any]:
        """Parse Claude's JSON response."""
        try:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group())
            else:
                analysis = json.loads(response_text)

            # Validate required fields
            score = int(analysis.get("score", 50))
            temperature = analysis.get("temperature", "warm").lower()

            # Ensure temperature is valid
            if temperature not in ["hot", "warm", "cold"]:
                temperature = "warm"

            return {
                "score": max(0, min(100, score)),  # Clamp to 0-100
                "temperature": temperature,
                "reasoning": analysis.get("reasoning", ""),
                "action": analysis.get("action", ""),
                "budget_estimate": analysis.get("budget_estimate"),
                "timeline_estimate": analysis.get("timeline_estimate")
            }

        except Exception as e:
            logger.error(f"Failed to parse Claude response: {e}")
            # Fallback to default
            return {
                "score": 50,
                "temperature": "warm",
                "reasoning": "Error parsing AI response",
                "action": "Manual review required",
                "budget_estimate": None,
                "timeline_estimate": None
            }

    async def _persist_lead(self, contact_id: Optional[str], lead_data: Dict[str, Any], analysis: Dict[str, Any]) -> None:
        if not contact_id:
            return
        try:
            await upsert_contact(
                contact_id=contact_id,
                location_id=lead_data.get("location_id") or lead_data.get("locationId"),
                name=lead_data.get("name") or lead_data.get("contact", {}).get("name"),
                email=lead_data.get("email") or lead_data.get("contact", {}).get("email"),
                phone=lead_data.get("phone") or lead_data.get("contact", {}).get("phone"),
            )
            await upsert_lead(
                contact_id=contact_id,
                location_id=lead_data.get("location_id") or lead_data.get("locationId"),
                score=analysis.get("score"),
                temperature=analysis.get("temperature"),
                budget_min=analysis.get("budget_min"),
                budget_max=analysis.get("budget_max"),
                timeline=analysis.get("timeline_estimate"),
                service_area_match=analysis.get("service_area_match"),
                is_qualified=analysis.get("meets_jorge_criteria"),
                metadata_json={
                    "source": lead_data.get("source"),
                    "tags": lead_data.get("tags", []),
                    "jorge_validation": analysis.get("jorge_validation"),
                },
            )
        except Exception as e:
            logger.warning(f"Failed to persist lead {contact_id}: {e}")

    async def _update_ghl_fields(self, contact_id: str, analysis: Dict[str, Any]):
        """Update GHL custom fields with analysis results."""
        try:
            result = await self._async_ghl_update(contact_id, analysis)
            success = result.get("success", False)

            if success:
                logger.info(f"âœ… Updated GHL fields for {contact_id}")
            else:
                logger.warning(f"âš ï¸ GHL update failed: {result.get('error')}")

            # Emit GHL fields updated event
            try:
                await event_broker.publish_lead_event(
                    "lead.ghl_updated",
                    contact_id=contact_id,
                    fields_updated=["ai_lead_score", "lead_temperature"],
                    update_success=success,
                    error_message=result.get("error") if not success else None
                )
            except Exception as e:
                logger.warning(f"Failed to publish GHL update event: {e}")

        except Exception as e:
            logger.error(f"GHL update error: {e}")

            # Emit GHL update error event
            try:
                await event_broker.publish_lead_event(
                    "lead.ghl_updated",
                    contact_id=contact_id,
                    fields_updated=[],
                    update_success=False,
                    error_message=str(e)
                )
            except Exception as event_error:
                logger.warning(f"Failed to publish GHL update error event: {event_error}")

    async def _async_ghl_update(self, contact_id: str, analysis: Dict[str, Any]) -> Dict:
        """Update GHL lead score fields using async client method."""
        return await self.ghl.update_lead_score(
            contact_id,
            analysis["score"],
            analysis["temperature"],
        )

    async def _send_followup(self, contact_id: str, analysis: Dict[str, Any]):
        """Send immediate follow-up based on lead temperature."""
        try:
            temperature = analysis["temperature"]
            result = await self._async_send_followup(contact_id, temperature)
            success = result.get("success", False)

            if success:
                logger.info(f"ðŸ“¬ Follow-up sent for {contact_id} ({temperature})")

            # Emit follow-up sent event
            try:
                await event_broker.publish_lead_event(
                    "lead.followup_sent",
                    contact_id=contact_id,
                    temperature=temperature,
                    message_type="sms",  # Could be dynamic based on GHL config
                    message_sent=success
                )
            except Exception as e:
                logger.warning(f"Failed to publish follow-up sent event: {e}")

        except Exception as e:
            logger.error(f"Follow-up error: {e}")

            # Emit follow-up error event
            try:
                await event_broker.publish_lead_event(
                    "lead.followup_sent",
                    contact_id=contact_id,
                    temperature=analysis.get("temperature", "unknown"),
                    message_type="sms",
                    message_sent=False
                )
            except Exception as event_error:
                logger.warning(f"Failed to publish follow-up error event: {event_error}")

    async def _async_send_followup(self, contact_id: str, temperature: str) -> Dict:
        """Send follow-up using async client method."""
        return await self.ghl.send_immediate_followup(contact_id, temperature)

    def _fallback_scoring(self, lead_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Fallback scoring if AI fails.

        Simple rule-based scoring for reliability.
        """
        score = 50  # Default medium
        temperature = "warm"

        # Check if we have basic contact info
        has_email = bool(lead_data.get("email"))
        has_phone = bool(lead_data.get("phone"))

        if has_email and has_phone:
            score = 60
        elif has_email or has_phone:
            score = 50
        else:
            score = 30
            temperature = "cold"

        return {
            "score": score,
            "temperature": temperature,
            "reasoning": "Fallback scoring (AI unavailable)",
            "action": "Manual review required",
            "budget_estimate": None,
            "timeline_estimate": None
        }
